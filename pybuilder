#!/usr/bin/python3
#
# This is a simplistic build script for building python wheels
# natively (ie using pip) and upload the resulting artefacts to a
# local repo.
#
#   Copyright (C) 2020 Olaf Kirch <okir@suse.de>
#
#   This program is free software; you can redistribute it and/or modify
#   it under the terms of the GNU General Public License as published by
#   the Free Software Foundation; either version 2 of the License, or
#   (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
#

import sys
import os
import os.path
import io
import pkginfo
import glob
import shutil

def run_command(cmd):
	print("Running %s" % cmd)
	if os.system(cmd) != 0:
		raise ValueError("Command `%s' returned non-zero exit status" % cmd)

def getinfo_pkginfo(path):

	if path.endswith(".whl"):
		return pkginfo.Wheel(path)

	if os.path.isdir(path):
		return pkginfo.UnpackedSDist(path)

	return pkginfo.SDist(path)
	
def getinfo_setuptools(path):
	path = os.path.join(path, 'setup.py')
	if not os.path.exists(path):
		return None

	setup_args = None

	def my_setup(**kwargs):
		nonlocal setup_args
		setup_args = kwargs

	import setuptools

	setuptools.setup = my_setup

	import importlib.util
	spec = importlib.util.spec_from_file_location('setup', path)
	mod = importlib.util.module_from_spec(spec)
	spec.loader.exec_module(mod)

	print("setup() called with args", setup_args.keys())

	mapping = {
		'author' : 'author',
		'author_email' : 'author_email',
		'classifiers' : 'classifiers',
		'keywords' : 'keywords',
		'license' : 'license',
		'url' : 'home_page',
		'description' : 'summary',
		'long_description' : 'description',
		'long_description_content_type' : 'description_content_type',

		'python_requires' : 'requires_python',
		'setup_requires' : 'requires',
		# 'tests_require' : 'requires',
		# 'extras_require' : 'requires'
	}

	d = pkginfo.Distribution()
	for (key, attr) in mapping.items():
		value = setup_args.get(key)
		if value:
			existing = getattr(d, attr, None)
			if existing is not None:
				if type(existing) == list or type(value) == list:
					value = list(existing) + list(value)
				else:
					value = existing + value
			setattr(d, attr, value)

	return d

def pkginfo_as_dict(pkg):
	attrs = []
	for a in dir(pkg):
		if not a.startswith("_"):
			attrs.append(a)

	result = dict()
	for k in attrs:
		v = getattr(pkg, k, None)
		if v is None:
			continue
		if callable(v):
			continue
		result[k] = v

	return result

def pkginfo_print(pkg):
	attrs = []
	for a in dir(pkg):
		if not a.startswith("_"):
			attrs.append(a)

	for k in attrs:
		v = getattr(pkg, k, None)
		if v is None:
			continue
		if callable(v):
			continue

		k = k.capitalize() + ":"
		if type(v) not in (tuple, list):
			v = repr(v)
			if len(v) > 100:
				v = v[:100] + "..."
			print("%-20s  %s" % (k, v))
		elif len(v) == 0:
			# print("%-20s  %s" % (k, "<empty>"))
			continue
		else:
			print("%s" % (k))
			for vi in v:
				print("        %s" % vi)

def pkginfo_as_metadata(pkg):
	def name_to_header(s):
		# strip off trailing plural "s"
		if s in ('classifiers', 'supported_platforms', 'project_urls'):
			s = s[:-1]
		s = (n.capitalize() for n in s.split('_'))
		return "-".join(s)

	def maybe_print(key):
		nonlocal info
		nonlocal seen

		header = name_to_header(key)
		seen.add(key)

		res = header + ": "

		value = info.get(key)
		if value is None:
			return ""

		if type(value) not in (tuple, list):
			if type(value) != str:
				value = repr(value)

			if '\n' in value:
				res += "\n        ".join(value.split('\n'))
			else:
				res += value
		else:
			# Turn classifiers list into several "Classifier:" header lines
			if "Url" in header:
				header = header.replace('Url', "URL")

			res = "\n".join("%s: %s" % (header, v) for v in value)

		if res:
			res += "\n"
			# print(">> %s" % res)
		return res

	seen = set()
	first = ('metadata_version', 'name', 'version', 'summary', 'home_page', 'author', 'author_email', 'maintainer', 'maintainer_email', 'license')
	ignore = ('filename', 'description', 'description_content_type')

	info = pkginfo_as_dict(pkg)

	result = ""
	for key in first:
		result += maybe_print(key)

	for key in sorted(info.keys()):
		if key in seen or key in ignore:
			continue
		result += maybe_print(key)

	result += maybe_print('description_content_type')

	if pkg.description:
		result += "\n" + pkg.description

	return result

def get_python_version():
	vi = sys.version_info
	return "%d.%d.%d" % (vi.major, vi.minor, vi.micro)

def canonical_package_name(name):
	return name.replace('_', '-')

class PackageBuildInfo(object):
	def __init__(self, name, version = None, type = None):
		self.name = name.replace('_', '-')
		self.version = version
		self.type = type
		self.requires_python = None

		self.fullreq = None
		self.url = None
		self.filename = None
		self.local_path = None
		self.hash = {}

	def id(self):
		if not self.version:
			return self.name
		return "%s-%s" % (self.name, self.version)

	def add_hash(self, algo, md):
		# print("%s %s=%s" % (self.filename, algo, md))
		self.hash[algo] = md

	def update_hash(self, algo):
		import hashlib

		m = hashlib.new(algo)
		with open(self.local_path, "rb") as f:
			m.update(f.read())

		self.add_hash(algo, m.hexdigest())

	def verify_requires_python(self):
		# We could also use Marker('python_version %s').evaluate()
		from packaging.specifiers import SpecifierSet, InvalidSpecifier

		requires_python = self.requires_python
		if requires_python is None:
			return True

		try:
			spec = SpecifierSet(requires_python, prereleases = False)
		except InvalidSpecifier:
			print("Warning: %s has invalid requires_python specifier \"%s\"" % (self.id(), requires_python))
			return False

		return get_python_version() in spec

	@staticmethod
	def parse_filename(filename):
		# try to detect version and type by looking at the file name
		_name = filename
		if filename.endswith(".tar.gz"):
			_name = filename[:-7]
			_name, _version = _name.rsplit('-', 1)
			_type = 'sdist'
		elif filename.endswith(".zip"):
			_name = filename[:-4]
			_name, _version = _name.rsplit('-', 1)
			_type = 'sdist'
		elif filename.endswith(".whl"):
			_name, _version, _crap = filename.split('-', 2)
			_type = 'bdist_wheel'
		else:
			raise ValueError("Unable to parse file name \"%s\"" % _name)

		return _name, _version, _type

	@staticmethod
	def from_local_file(path, name = None, version = None, type = None):
		filename = os.path.basename(path)

		if not name or not version or not type:
			# try to detect version and type by looking at the file name
			_name, _version, _type = PackageBuildInfo.parse_filename(filename)

			if name:
				assert(name == _name)
			name = _name
			if version:
				assert(version == _version)
			version = _version
			if type:
				assert(type == _type)
			type = _type

		build = PackageBuildInfo(name, version, type)
		build.filename = filename
		build.local_path = path

		build.update_hash('md5')
		build.update_hash('sha256')

		return build

class PackageReleaseInfo(object):
	def __init__(self, name, version, parsed_version = None):
		self.name = canonical_package_name(name)
		self.version = version

		if not parsed_version:
			from packaging.specifiers import parse
			parsed_version = parse(version)

		self.parsed_version = parsed_version

		self.builds = []

	def id(self):
		return "%s-%s" % (self.name, self.version)

	def more_recent_than(self, other):
		assert(isinstance(other, PackageReleaseInfo))
		return other.parsed_version < this.parsed_version

	def add_build(self, build):
		self.builds.append(build)

	def build_types(self):
		return [b.type for b in self.builds]

class PackageInfo(object):
	def __init__(self, name):
		self.name = canonical_package_name(name)
		self.releases = []

	def add_release(self, release):
		self.releases.append(release)

	def versions(self):
		return [r.version for r in self.releases]

class DownloadFinder(object):
	def __init__(self, req_string, verbose):
		from packaging.requirements import Requirement

		req = Requirement(req_string)
		self.requirement = req

		self.name = req.name
		self.allow_prereleases = False
		self.request_specifier = req.specifier
		self.verbose = verbose

	def release_match(self, release):
		assert(release.parsed_version)
		if not self.allow_prereleases and release.parsed_version.is_prerelease:
			return False

		if self.request_specifier and release.parsed_version not in self.request_specifier:
			return False

		return True

	def get_best_match(self, index):
		info = index.get_package_info(self.name)

		if self.verbose:
			print("%s versions: %s" % (self.name, ", ".join(info.versions())))

		best_ver = None
		best_match = None

		for release in info.releases:
			if not self.release_match(release):
				if self.verbose:
					print("ignoring release %s" % release.id())
				continue

			if best_ver and release.parsed_version <= best_ver:
				continue

			for build in release.builds:
				# print("%s: inspecting build %s" % (release.id(), build.filename))
				if self.build_match(build):
					if not build.verify_requires_python():
						if self.verbose:
							print("ignoring build %s (incompatible python requirement)" % build.filename)
						continue

					best_match = build
					best_ver = release.parsed_version

		if not best_match:
			raise ValueError("%s: unable to find a release that is compatible with my python version" % self.name)

		if self.verbose:
			print("Using %s" % best_match.id())

		return best_match

class SourceDownloadFinder(DownloadFinder):
	def __init__(self, req_string, verbose = False):
		super(SourceDownloadFinder, self).__init__(req_string, verbose)

	def build_match(self, build):
		# print("inspecting %s which is of type %s" % (build.filename, build.type))
		return build.type == 'sdist'

class BinaryDownloadFinder(DownloadFinder):
	def __init__(self, req_string, verbose = False):
		super(BinaryDownloadFinder, self).__init__(req_string, verbose)

	def build_match(self, build):
		# print("inspecting %s which is of type %s" % (build.filename, build.type))
		return build.type == 'bdist_wheel'

class PackageIndex(object):
	def __init__(self, url):
		self.url = url
		self._pkg_url_template = None

	def get_package_info(self, name):
		import urllib.request

		url = self._pkg_url_template.format(index_url = self.url, pkg_name = name)

		resp = urllib.request.urlopen(url)
		if resp.status != 200:
			raise ValueError("Unable to get package info for %s: HTTP response %s (%s)" % (
					name, resp.status, resp.reason))

		return self.process_package_info(name, resp)

class JSONPackageIndex(PackageIndex):
	def __init__(self, url = 'https://pypi.org/pypi'):
		super(JSONPackageIndex, self).__init__(url)
		self._pkg_url_template = "{index_url}/{pkg_name}/json"

	# JSON contains
	# ['info', 'last_serial', 'releases', 'urls']
	# info is a dict of PKG-INFO stuff
	# last_serial is an int, not sure what for
	def process_package_info(self, name, resp):
		from packaging.specifiers import parse
		import json

		info = PackageInfo(name)

		d = json.load(resp)
		for (ver, files) in d['releases'].items():
			relInfo = PackageReleaseInfo(name, ver)
			info.add_release(relInfo)

			for download in files:
				filename = download['filename']

				if download['yanked']:
					print("Ignoring %s: yanked (%s)" % (filename, download['yanked_reason']))
					continue

				type = download['packagetype']
				if type == 'sdist':
					if download['python_version'] != 'source':
						raise ValueError("%s: unable to deal with sdist package w/ version \"%s\"" % (download['filename'], download['python_version']))
					pkgInfo = PackageBuildInfo(name, ver, type = type)
				elif type.startswith('bdist'):
					pkgInfo = PackageBuildInfo(name, ver, type = type)
				else:
					print("%s: don't know how to deal with package type \"%s\"" % (filename, type))
					continue

				pkgInfo.filename = filename
				pkgInfo.url = download['url']
				pkgInfo.requires_python = download['requires_python']

				md = download.get('md5_digest')
				if md:
					pkgInfo.add_hash('md5', md)
				for algo, md in download['digests'].items():
					pkgInfo.add_hash(algo, md)

				relInfo.add_build(pkgInfo)

		return info

class SimplePackageIndex(PackageIndex):
	def __init__(self, url):
		super(SimplePackageIndex, self).__init__(url)

		# The trailing / is actually important, because otherwise
		# urljoin(base_url, "../../packages/blah/fasel")
		# will not do the right thing
		self._pkg_url_template = "{index_url}/simple/{pkg_name}/"

	# HTML response contains
	# <head>...
	#  <meta name="api-version" value="2"/>
	# </head>
	# <body><h1>Links for flit</h1>
        # <a href="../../packages/flit/0.1/$filename#sha256=$hexdigest" rel="internal" data-requires-python="3" >$filename</a><br/>
	# ...
	def process_package_info(self, name, resp):
		import xml.etree.ElementTree as ET
		tree = ET.parse(resp)
		root = tree.getroot()

		info = None
		if root.tag == 'html':
			for node in root:
				if node.tag == 'head':
					self.process_html_head(node)
				elif node.tag == 'body':
					if info:
						raise ValueError("Server response contains several <body> elements")
					info = self.process_html_body(resp.url, node)
		else:
			info = self.process_html_body(resp.url, root)

		if not info:
			raise ValueError("Server response lacks a <body> element")

		return info

	def process_html_head(self, node):
		for m in node.findall('meta'):
			name = m.attrib.get('name')
			if name != 'api-version':
				continue

			value = m.attrib.get('value')
			if value != '2':
				raise ValueError("Unable to deal with pypi simple index API version %s" % value)

			# print("Found API version %s; good" % value)

		# all is well

	def process_html_body(self, request_url, node):
		builds = []

		for anchor in node.findall(".//a"):
			build = self.process_html_a(request_url, anchor)
			if build:
				builds.append(build)

		if not builds:
			raise ValueError("No <a> elements found in server response")

		name = builds[0].name
		for b in builds:
			if b.name != name:
				raise ValueError("Server response contains a mix of packages (%s and %s)" % (name, b.name))

		releases = dict()
		for b in builds:
			r = releases.get(b.version)
			if not r:
				r = PackageReleaseInfo(name, b.version)
				releases[r.version] = r

			r.add_build(b)

		info = PackageInfo(name)
		for release in sorted(releases.values(), key = lambda r: r.parsed_version):
			info.add_release(release)

		return info

	def process_html_a(self, request_url, anchor):
		from urllib.parse import urljoin, urldefrag

		rel = anchor.attrib.get('rel')
		if rel != "internal":
			print("IGNORING anchor with rel=%s" % rel)
			return None

		href = anchor.attrib['href']
		href, hash = urldefrag(href)

		# FIXME: this only works on systems that use '/' as the path separator
		filename = os.path.basename(href)
		assert(filename == anchor.text)

		try:
			name, version, type = PackageBuildInfo.parse_filename(filename)
		except ValueError:
			return None

		build = PackageBuildInfo(name, version, type)
		build.filename = filename

		build.url = urljoin(request_url, href)
		# print("%s plus %s -> %s" % (request_url, href, build.url))

		rpy = anchor.attrib.get('data-requires-python')
		if rpy:
			build.requires_python = rpy

		if hash:
			algo, md = hash.split('=')
			build.add_hash(algo, md)

		return build


# For now, this is a very trivial downloader.
# This could be something much more complex that uses caches, OBS, yadda yadda
class Downloader(object):
	def __init__(self):
		pass

	def download(self, build):
		import urllib.request

		if build.local_path:
			return build.local_path

		assert(build.url)
		assert(build.filename)

		url = build.url
		resp = urllib.request.urlopen(url)
		if resp.status != 200:
			raise ValueError("Unable to download %s from %s (HTTP status %s %s)" % (
					build.filename, url, resp.status, resp.reason))

		filename = build.filename
		with open(filename, "wb") as f:
			f.write(resp.read())

		print("Downloaded %s from %s" % (filename, url))

		build.local_path = filename
		return filename

# For now, a very trivial uploader.
class Uploader(object):
	def __init__(self, repo):
		self.repo = repo

	def describe(self):
		return "Python repository \"%s\"" % self.repo

	def upload(self, build):
		assert(build.local_path)

		print("Uploading %s to %s repository" % (build.local_path, self.repo))
		cmd = "twine upload --verbose "
		cmd += " --disable-progress-bar"
		cmd += " --repository %s %s" % (self.repo, build.local_path)

		print("Running %s" % cmd)
		if os.system(cmd) != 0:
			raise ValueError("Command `%s' returned non-zero exit status" % cmd)

class WheelArchive(object):
	def __init__(self, path):
		self.path = path
		self._zip = self.open()

	def open(self):
		import zipfile

		return zipfile.ZipFile(self.path, mode = 'r')

	@property
	def basename(self):
		return os.path.basename(self.path)

	def name_set(self):
		result = set()
		for member in self._zip.infolist():
			if member.is_dir():
				continue

			result.add(member.filename)

		return result

	def compare(self, other):
		my_name_set = self.name_set()
		other_name_set = other.name_set()

		added_set = other_name_set - my_name_set
		removed_set = my_name_set - other_name_set

		changed_set = set()
		for member_name in my_name_set.intersection(other_name_set):
			my_data = self._zip.read(member_name)
			other_data = other._zip.read(member_name)

			if other_data != my_data:
				changed_set.add(member_name)

		if False:
			print("added=" + ", ".join(added_set))
			print("removed=" + ", ".join(removed_set))
			print("changed=" + ", ".join(changed_set))

		return added_set, removed_set, changed_set

class Package(object):
	def __init__(self, build):
		assert(build.filename)

		self._source = build
		self._info = getinfo_pkginfo(build.local_path)

		self.build_dir = "BUILD"
		self._unpacked_dir = None
		self.build_requires = []
		self.artefacts = []

	@property
	def name(self):
		return self._info.name

	@property
	def version(self):
		return self._info.version

	@property
	def path(self):
		return self._source.local_path

	@property
	def NV(self):
		return "%s-%s" % (self.name, self.version)

	def get_hash(self, md):
		import hashlib

		m = hashlib.new(md)
		with open(self.local_path, "rb") as f:
			m.update(f.read())

		return m.hexdigest()

	def print_info(self):
		pkginfo_print(self._info)

	def run(self, cmd):
		print("Running %s" % cmd)
		if os.system(cmd) != 0:
			raise ValueError("Command `%s' returned non-zero exit status" % cmd)

class Builder(object):
	def __init__(self, build_dir, state_dir, prefer_git = False):
		self.build_dir = build_dir
		self.state_dir = state_dir
		self.prefer_git = prefer_git

	def unpack(self, sdist):
		archive = sdist.local_path
		if not archive or not os.path.exists(archive):
			raise ValueError("Unable to unpack %s: no local copy" % sdist.filename)

		if os.path.exists(self.build_dir):
			shutil.rmtree(self.build_dir)

		unpacked_dir = None
		if self.prefer_git:
			unpacked_dir = self.try_unpack_git(sdist)

		if not unpacked_dir:
			shutil.unpack_archive(archive, self.build_dir)
			unpacked_dir = os.path.join(self.build_dir, sdist.id())

		print("Unpacked %s to %s" % (archive, unpacked_dir))
		return BuildDirectory(sdist, unpacked_dir)

	def try_unpack_git(self, sdist):
		repo_url = sdist.git_url()
		if not repo_url:
			return None

		return self.unpack_git(repo_url)

	def unpack_git(self, git_repo):
		git_repo = self._info.home_page
		git_repo = git_repo.replace('_', '-')
		if not git_repo or not "github.com" in git_repo:
			raise ValueError("Package homepage \"%s\" doesn't look like a git repo" % git_repo)

		unpacked_dir = os.path.join(self.build_dir, sdist.id())
		self.run("git clone %s %s" % (git_repo, unpacked_dir))

		cwd = os.getcwd()
		try:
			os.chdir(self._unpacked_dir)
			run_command("git checkout %s" % self.version)
		finally:
			os.chdir(cwd)

		return unpacked_dir

	def make_statedir(self, sdist):
		savedir = os.path.join(self.state_dir, sdist.id())
		return PythonBuildState(savedir)

class BuildDirectory(object):
	def __init__(self, sdist, unpacked_dir):
		self.sdist = sdist
		self.unpacked_dir = unpacked_dir
		self.quiet = False

		self.artefacts = []

	def build(self, quiet = False):
		assert(self.unpacked_dir)

		cwd = os.getcwd()
		try:
			os.chdir(self.unpacked_dir)
			return self._do_build()
		finally:
			os.chdir(cwd)

	def _do_build(self):
		sdist = self.sdist

		cmd = "python3 setup.py bdist_wheel"
		cmd = "pip3 wheel --wheel-dir dist ."
		cmd += " --log pip.log"
		cmd += " --no-deps"

		if self.quiet:
			cmd += " >build.log 2>&1"
		else:
			cmd += " 2>&1 | tee build.log"

		run_command(cmd)

		wheels = glob.glob("dist/*.whl")
		print("Successfully built %ss: %s" % (sdist.id(), ", ".join(wheels)))

		for w in wheels:
			w = os.path.join(os.getcwd(), w)

			build = PackageBuildInfo.from_local_file(w)

			for algo in ('md5', 'sha256'):
				build.update_hash(algo)

			self.artefacts.append(build)

		return self.artefacts

	def unchanged_from_previous_build(self, build_state):
		samesame = True
		for wheel in self.artefacts:
			wheel_name = os.path.basename(wheel.local_path)

			old_wheel_path = build_state.get_old_path(wheel_name)
			old_wheel_path = os.path.join(savedir, wheel_name)
			print("Checking %s vs %s" % (wheel.local_path, old_wheel_path))
			if not os.path.exists(old_wheel_path):
				print("%s does not exist" % old_wheel_path)
				samesame = False
				continue

			old_wheel = WheelArchive(old_wheel_path)
			new_wheel = WheelArchive(wheel.local_path)
			if not self.wheels_identical(old_wheel, new_wheel):
				print("%s differs from previous build" % wheel_name)
				samesame = False
				continue

		path = build_state.get_old_path("build-requires")
		if not os.path.exists(path):
			print("Previous build of %s did not write a build-requires file" % self.sdist.id())
			samesame = False
		else:
			build_req_string = self.build_requires_as_string()

			with open(path, "r") as f:
				if f.read() != build_req_string:
					print("Build requirements changed")
					samesame = False

		return samesame

	def wheels_identical(self, old_wheel, new_wheel):
		def print_delta(wheel, how, name_set):
			print("%s: %s %d file(s)" % (wheel.basename, how, len(name_set)))
			for name in name_set:
				print("  %s" % name)

		added_set, removed_set, changed_set = old_wheel.compare(new_wheel)

		samesame = True
		if added_set:
			print_delta(new_wheel, "added", added_set)
			samesame = False

		if removed_set:
			print_delta(new_wheel, "removed", removed_set)
			samesame = False

		if changed_set:
			print_delta(new_wheel, "changed", changed_set)
			samesame = False

		if samesame:
			print("%s: unchanged" % new_wheel.basename)

		return samesame

	# There must be a smarter way to extract the build requirements than
	# this...
	def guess_build_dependencies(self):
		from packaging.requirements import Requirement
		import re

		logfile = os.path.join(self.unpacked_dir, "pip.log")
		if not os.path.exists(logfile):
			return

		print("Parsing %s" % logfile)
		req = None

		self.build_requires = []
		with open(logfile, "r") as f:
			for l in f.readlines():
				# Parse lines like:
				# Added flit_core<4,>=3.0.0 from http://.../flit_core-3.0.0-py3-none-any.whl#md5=7648384867c294a95487e26bc451482d to build tracker
				if req and ('Added' in l) and ('to build tracker' in l):
					from urllib.parse import urldefrag, urlparse

					m = re.search('Added ([^ ]*) from ([^ ]*)', l)
					if not m:
						raise ValueError("regex match failed")

					r = Requirement(m.group(1))
					url = m.group(2)

					# This is needed to deal with some oddities of pip.
					# For instance, package flit requires flit_core. However,
					# pip will change that name to flit-core, and look for it
					# in the index by this name.
					# pypi has both flit_core and flit-core wheels, but they have
					# different hash digests, causing our rebuild checks to
					# fire without need.
					if req.name != r.name:
						r.name = canonical_package_name(r.name)
						assert(req.name == r.name)
					req.fullreq = str(r)

					url, frag = urldefrag(url)
					if frag:
						algo, md = frag.split('=')
						req.add_hash(algo, md)

					req.url = url
					req.filename = os.path.basename(urlparse(url).path)
					continue

				if "to search for versions of" not in l:
					continue

				m = re.search('to search for versions of (.*):', l)
				if not m:
					raise ValueError("regex match failed")

				req = PackageBuildInfo(m.group(1))
				self.build_requires.append(req)
				if not self.quiet:
					print("Found requirement %s" % req.name)

	def prepare_results(self, build_state):
		self.maybe_save_file(build_state, "build.log")
		self.maybe_save_file(build_state, "pip.log")

		build_state.write_file("build-requires", self.build_requires_as_string())
		build_state.write_file("build-artefacts", self.build_artefacts_as_string())

		for wheel in self.artefacts:
			# Copy the wheel itself
			wheel.local_path = build_state.save_file(wheel.local_path)

			name = "%s-METADATA.txt" % wheel.id()
			pi = getinfo_pkginfo(wheel.local_path)
			build_state.write_file(name,
				pkginfo_as_metadata(pi),
				"%s metadata" % wheel.id())

	def build_artefacts_as_string(self):
		b = io.StringIO()
		for wheel in self.artefacts:
			b.write("wheel %s\n" % wheel.name)
			b.write("  version %s\n" % wheel.version)

			for algo in ('md5', 'sha256'):
				b.write("  hash %s %s\n" % (algo, wheel.hash.get(algo)))
		return b.getvalue()

	def build_requires_as_string(self):
		b = io.StringIO()
		for req in self.build_requires:
			b.write("require %s\n" % req.name)
			if req.fullreq:
				b.write("  specifier %s\n" % req.fullreq);
			if req.hash:
				for (algo, md) in req.hash.items():
					b.write("  hash %s %s\n" % (algo, md))
		return b.getvalue()

	def maybe_save_file(self, build_state, name):
		path = os.path.join(self.unpacked_dir, name)

		if not os.path.exists(path):
			print("Not saving %s (does not exist)" % path)
			return None

		return build_state.save_file(path)

	def write_file(self, build_state, name, write_func):
		buffer = io.StringIO()
		write_func(buffer)
		return build_state.write_file(name, buffer.getvalue())

	def cleanup(self):
		pass

class BuildState(object):
	def __init__(self, savedir):
		import tempfile

		self.savedir = savedir
		self.tmpdir = tempfile.TemporaryDirectory(prefix = "pybuild-")

	def __del__(self):
		self.cleanup()

	def commit(self):
		if not self.tmpdir:
			print("%s: changes already committed" % self.savedir)
			return

		# Clean up the savedir
		if os.path.exists(savedir):
			shutil.rmtree(savedir)
		os.makedirs(savedir, mode = 0o755)

		# And copy our data over it
		print("Committing build state to %s:" % self.savedir, end = ' ')
		for file in glob.glob(os.path.join(self.tmpdir.name, "*")):
			print(os.path.basename(file), end = ' ')
			shutil.copy(file, savedir)
		print("")

	def cleanup(self):
		if self.tmpdir:
			del self.tmpdir
		self.tmpdir = None

	def get_old_path(self, name):
		return os.path.join(self.savedir, name)

	def maybe_save_file(self, src, dst = None):
		if not os.path.exists(src):
			print("Not saving %s (does not exist)" % src)
			return None

		return self.save_file(src, dst)

	def save_file(self, src):
		dst = self.tmpdir.name

		print("Saving %s to %s" % (src, dst))
		shutil.copy(src, dst)

		if os.path.isdir(dst):
			return os.path.join(dst, os.path.basename(src))
		return dst

	def write_file(self, name, data, desc = None):
		path = os.path.join(self.tmpdir.name, name)

		if desc:
			print("Writing %s to %s" % (desc, path))
		else:
			print("Writing %s" % path)
		with open(path, "w") as f:
			f.write(data)

class PythonBuildState(BuildState):
	def __init__(self, savedir):
		super(PythonBuildState, self).__init__(savedir)

	def rebuild_required(self, index):
		path = self.get_old_path("build-artefacts")
		if not os.path.exists(path):
			return True

		path = self.get_old_path("build-requires")
		if not os.path.exists(path):
			return True

		try:
			req_list = self.parse_build_requires(path)
		except:
			print("Cannot parse build-requires file at %s" % path)
			return True

		for req in req_list:
			if req.fullreq:
				finder = BinaryDownloadFinder(req.fullreq)
			else:
				finder = BinaryDownloadFinder(req.name)

			print("Build requires %s" % (finder.requirement))

			p = finder.get_best_match(index)

			print("  Best match available from package index: %s" % p.filename)
			if req.version:
				if req.version != p.version:
					print("Building would pick %s-%s rather than %s-%s" % (
						p.name, p.version,
						req.name, req.version))
					return True

			match = False
			for algo, md in req.hash.items():
				print("  We are looking for %s %s %s" % (req.id(), algo, md))
				have_md = p.hash.get(algo)
				if have_md is None:
					print("  => index does not provide %s hash for %s" % (algo, p.filename))
					continue

				print("  => index provides %s %s %s" % (p.filename, algo, p.hash.get(algo)))
				if have_md == md:
					match = True

			if not match:
				print("%s was apparently rebuilt in the meantime, we need to rebuild" % p.filename)
				return True

			print("Build requirement did not change")

		return False

	def parse_build_requires(self, path):
		result = []

		with open(path, 'r') as f:
			req = None
			for l in f.readlines():
				if not l:
					continue

				if l.startswith("require"):
					name = l[7:].strip()
					req = PackageBuildInfo(name)
					result.append(req)
					continue

				if req is None:
					raise ValueError("%s: no build info in this context" % (path, ))

				if l.startswith(' '):
					words = l.strip().split()
					if not words:
						continue
					if words[0] == 'specifier':
						req.fullreq = words[1]
						continue

					if words[0] == 'hash':
						req.add_hash(words[1], words[2])
						continue

				raise ValueError("%s: unparseable line <%s>" % (path, l))

		return result

def build_option_parser():
	import optparse

	parser = optparse.OptionParser(usage = "blah")
	parser.add_option('--debug', default = False, action = 'store_true',
                help = "Enable debugging output")
	parser.add_option('--quiet', default = False, action = 'store_true',
                help = "Be less verbose")
	parser.add_option('--git', default = False, action = 'store_true',
                help = "Build from git repo rather than sdist")
	parser.add_option('--rebuild-if-needed', default = False, action = 'store_true',
                help = "Only rebuild package if needed")
	parser.add_option('--force', default = False, action = 'store_true',
                help = "Always store build results, even if they did not change")

	parser.add_option('--output-dir', default = "OUTPUT",
		help = "Directory to write output files to (defaults to OUTPUT)")
	parser.add_option('--upload-to', default = None,
		help = "Repository to upload packages to")

	return parser

(opts, args) = build_option_parser().parse_args()

if True:
	index_url = 'http://localhost:8081/repository/pypi-group/'
	packageIndex = SimplePackageIndex(url = index_url)
else:
	packageIndex = JSONPackageIndex()

downloader = Downloader()

uploader = None
if opts.upload_to:
	uploader = Uploader(opts.upload_to)

builder = Builder("BUILD", opts.output_dir, prefer_git = opts.git)

for file in args:
	print("Examining %s" % file)

	if os.path.exists(file):
		sdist = PackageBuildInfo.from_local_file(file)

		if sdist.type != "sdist":
			print("ERROR: cannot build %s - not a source distribution" % sdist.filename)
			continue

		# DownloadFinder will never return an sdist that
		# requires a python version incompatible with ours.
		# For an sdist provided directly on the command line,
		# it might be nice to check for this explicitly, but that's a
		# lot of work. Not the least because many sdist tarballs
		# do not contain structured build information, but provide
		# a more or less messy setup.py
	else:
		finder = SourceDownloadFinder(file, verbose = True)
		sdist = finder.get_best_match(packageIndex)

	print("=== Package %s ===" %(sdist.id()))

	if not sdist.local_path:
		downloader.download(sdist)

	pkg = Package(sdist)

	try:
		savedir = os.path.join(opts.output_dir, pkg.NV)
		build_state = builder.make_statedir(sdist)

		if opts.rebuild_if_needed:
			print("=== Checking whether a rebuild is required ===")
			if not build_state.rebuild_required(packageIndex):
				print("=== No rebuild required for %s ===" %(pkg.NV))
				continue

		print("=== Unpacking %s ===" %(pkg.NV))
		unpacked_dir = builder.unpack(pkg._source)
		unpacked_dir.quiet = opts.quiet

		print("=== Building %s ===" %(pkg.NV))
		built = unpacked_dir.build()

		if not built:
			raise ValueError("%s: nothing got built" % file)

		print("=== Inspecting build dependencies ===")
		unpacked_dir.guess_build_dependencies()

		for req in unpacked_dir.build_requires:
			for algo in ('md5', 'sha256'):
				if req.hash.get(algo) is None:
					print("WARNING: No %s hash for %s" % (algo, req.name))
					downloader.download(req)
					req.update_hash(algo)

		if unpacked_dir.build_requires:
			print("Build requirements:")
			sys.stdout.write(unpacked_dir.build_requires_as_string())

		print("=== Storing build results ===")
		unpacked_dir.prepare_results(build_state)

		print("=== Checking whether artefacts changed from previous build ===")
		if not opts.force and unpacked_dir.unchanged_from_previous_build(build_state):
			print("%s artefacts have not changed since previous build" % pkg.NV)
			continue

		if uploader:
			print("=== Uploading build results to %s ===" % uploader.describe())
			for p in built:
				uploader.upload(p)

			build_state.commit()
		else:
			print("=== Leaving build results in %s ===" % savedir)
			sys.stdout.write(unpacked_dir.build_artefacts_as_string())

		unpacked_dir.cleanup()
		build_state.cleanup()

	except Exception as e:
		print("FAILED to build %s" % pkg.NV)
		import traceback
		print(e)
		traceback.print_tb(sys.exc_info()[2])
		traceback.format_exc()

print("=== Done ===")
